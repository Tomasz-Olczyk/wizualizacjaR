---
title: "Analiza i wizualizacja danych tekstowych cz 1"
author: "Tomasz Olczyk"
date: "11/7/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





## Biblioteki na początek

```{r}
library(tidyverse)
library(tidytext) # musimy doinstalować biblioteka do analizy danych tekstowych
```

## Dane 

```{r}
paski <-read.delim(file =url("https://raw.githubusercontent.com/Tomasz-Olczyk/wizualizacjaR/main/tekst/paski.txt"), header = FALSE, quote = "") 
 #parametr quote dodałem ze względu na to, że w pliku są słowa w cudzysłowie, bez tego parametru baza nie łąduje się w całosći
# header = FALSE oznacza że funkcja nie traktuje pierwszego wiersza jak nagłówwka

```


```{r}
debata  <- read.delim("https://raw.githubusercontent.com/Tomasz-Olczyk/wizualizacjaR/refs/heads/main/tekst/debata_us24.txt", sep = ":")

```

### Analiza debaty prezydenckiej  w USA

```{r}
glimpse(debata)
```


Wybierzmy obserwacje które sa tylko 

```{r}
kandydaci <- debata %>%
  filter(speakers == "HARRIS" | speakers == "TRUMP")
```


```{r}
text_df <- tibble(text = kandydaci$text)

#zamaina na format tibble ten krok nie jest konieczny

```

Tokenizujemy zbiór używają funkcji unnest_tokens:

```{r}
tokeny <- text_df %>%
  unnest_tokens(word, text,  token = "words")
```


Przyjrzyjmy się funkcji unnest_tokens
```{r}
?unnest_tokens
```

Tokenami mogą być np. zdania

```{r}
sentences <- text_df %>%
  unnest_tokens(sentence, text,  token = "sentences")
```

W ilościowej analizie tekstu i wizualizacji przydatne jest używanie stoplist: list słów często powtarząjcych się w danym języku, które dobrze jest odsiać z tekstu. Do tego celu przydać się na może biblioteka stopwords. 


```{r}
#install.packages("stopwords")
library(stopwords)
```


# usuwamy słowa ze stoplisty

```{r}

text_df_1 <- text_df %>% 
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(language = "en", source = "snowball"))
#anti_join jest złączeniem filturjącym
```


Zliczamy frekwencje słów

```{r}
text_frek <-
  as_tibble(text_df_1) %>% 
  group_by(word) %>%
  summarise(freq = n()) %>%
  distinct(word, .keep_all = TRUE) 
```


## Biblioteki do wzualizacji w postaci chmury

```{r}
library(ggwordcloud)
library(wordcloud2)
```

```{r}
wordcloud2(data = text_frek %>% filter(freq>10))
```




```{r}
ggplot(data = text_frek %>% filter(freq>20)) +
  geom_text_wordcloud(aes(label = word, 
                          size = freq), 
                      shape = "square",
                      rm_outside = FALSE, grid_size = 1,
                      eccentricity = .65)
```



### Zadanie 8

Stwórzmy chmurę słów porównującą obok siebie Trumpa i Harris


### Ngramy

```{r}

text_ngram <- text_df %>% unnest_tokens(ngrams, text, "ngrams", n = 2) 
```


```{r}
text_frek_n <-
  as_tibble(text_ngram) %>% 
  group_by(ngrams) %>%
  summarise(freq = n()) %>%
  distinct(ngrams, .keep_all = TRUE) %>%
  drop_na()
```

```{r}
wordcloud2(data = text_frek_n)
```

### Zadanie 9

Stwórzmy chmurę z ngramów trzyelementowych



## Drzewa słów

Pakiet korzysta z narzędzi google

Biblioteka googleVis oferuje możliwość wykorzystania wykresów google do tworzenia interaktywnych wykresów w html (otwierają się w przeglądarce) można je umieścić także w dokumencie r markdown jeśli odpowiednio skonstruuje się nagłówek yaml.

```{r, eval= FALSE}
#output:
 # html_document:
  #  self_contained: false
```

i ustawi

```{r}
#install.packages("googleVis")
```

Ładuję bibliotekę googleVis.

```{r}
library(googleVis)

```

Biblioteka Google Vis zasadniczo kreauje wizualizacje w przeglądarce ale można jej użyć także  w R markdown wykorzystując poniższą linię kodu



### Drzewo na bazie pasków z TVP


```{r}
kaczyński <- paski %>%
  filter(grepl("KACZYŃSKI", paski$V1))
```

```{r results='asis', include = TRUE}
wt2 <- gvisWordTree(kaczyński, textvar = "")

plot(wt2)

```



### Phrase net


```{r}
library(phrasenets) # trzeba zainstalować

```




```{r}


data(reuters)

phrasenets::phrase_net(reuters, text = text) %>% 
  head() %>% 
  knitr::kable()
```

```{r}
phrase_net(reuters$text[1:10]) %>% 
  plot_sigmajs()
```





```{r}
#install.packages("qdap")
```

```{r}
library(qdap)
```




```{r}
qdap::phrase_net(
  kaczyński$V1,
  freq = 1,
  r = 0.6,
  edge.constant = 6,
  vertex.constant = 3
)
```



### Zadanie 10 

stwórzmy phrase net dla słowa tariffs w pliku z debat


### W domu

Zainstalujmy bibliotekę spacyr

```{r}
library(spacyr)
```


użyjmy funkcji wobec danych z debaty

spacy_initialize() # uwaga trzeba ustawić odpowiedni model

a następnie
spacy_parse()


